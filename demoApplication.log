[main] INFO  top.xvzonghui.hadoop.fs.FileOperations  - now start to print content of file: /user/set_java_env.sh
[main] DEBUG org.apache.hadoop.util.Shell  - Failed to detect a valid hadoop home directory
java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:448)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:419)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:496)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2972)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:2968)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2830)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)
	at top.xvzonghui.hadoop.fs.FileOperations.cat(FileOperations.java:24)
	at top.xvzonghui.hadoop.fs.FileOperations.main(FileOperations.java:38)
[main] DEBUG org.apache.hadoop.util.Shell  - setsid is not available on this machine. So not using it.
[main] DEBUG org.apache.hadoop.util.Shell  - setsid exited with exit code 0
[main] DEBUG org.apache.hadoop.metrics2.lib.MutableMetricsFactory  - field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, about=, type=DEFAULT, value=[Rate of successful kerberos logins and latency (milliseconds)], valueName=Time)
[main] DEBUG org.apache.hadoop.metrics2.lib.MutableMetricsFactory  - field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, about=, type=DEFAULT, value=[Rate of failed kerberos logins and latency (milliseconds)], valueName=Time)
[main] DEBUG org.apache.hadoop.metrics2.lib.MutableMetricsFactory  - field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, about=, type=DEFAULT, value=[GetGroups], valueName=Time)
[main] DEBUG org.apache.hadoop.metrics2.lib.MutableMetricsFactory  - field private org.apache.hadoop.metrics2.lib.MutableGaugeLong org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailuresTotal with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, about=, type=DEFAULT, value=[Renewal failures since startup], valueName=Time)
[main] DEBUG org.apache.hadoop.metrics2.lib.MutableMetricsFactory  - field private org.apache.hadoop.metrics2.lib.MutableGaugeInt org.apache.hadoop.security.UserGroupInformation$UgiMetrics.renewalFailures with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, sampleName=Ops, about=, type=DEFAULT, value=[Renewal failures since last successful login], valueName=Time)
[main] DEBUG org.apache.hadoop.metrics2.impl.MetricsSystemImpl  - UgiMetrics, User and group related metrics
[main] DEBUG org.apache.hadoop.security.authentication.util.KerberosName  - Kerberos krb5 configuration not found, setting default realm to empty
[main] DEBUG org.apache.hadoop.security.Groups  -  Creating new Groups object
[main] DEBUG org.apache.hadoop.util.NativeCodeLoader  - Trying to load the custom-built native-hadoop library...
[main] DEBUG org.apache.hadoop.util.NativeCodeLoader  - Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path
[main] DEBUG org.apache.hadoop.util.NativeCodeLoader  - java.library.path=/Users/zonghuixu/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
[main] WARN  org.apache.hadoop.util.NativeCodeLoader  - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[main] DEBUG org.apache.hadoop.util.PerformanceAdvisory  - Falling back to shell based
[main] DEBUG org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback  - Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
[main] DEBUG org.apache.hadoop.security.Groups  - Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
[main] DEBUG org.apache.hadoop.security.UserGroupInformation  - hadoop login
[main] DEBUG org.apache.hadoop.security.UserGroupInformation  - hadoop login commit
[main] DEBUG org.apache.hadoop.security.UserGroupInformation  - using local user:UnixPrincipal: zonghuixu
[main] DEBUG org.apache.hadoop.security.UserGroupInformation  - Using user: "UnixPrincipal: zonghuixu" with name zonghuixu
[main] DEBUG org.apache.hadoop.security.UserGroupInformation  - User entry: "zonghuixu"
[main] DEBUG org.apache.hadoop.security.UserGroupInformation  - Assuming keytab is managed externally since logged in from subject.
[main] DEBUG org.apache.hadoop.security.UserGroupInformation  - UGI loginUser:zonghuixu (auth:SIMPLE)
[main] DEBUG org.apache.htrace.core.Tracer  - sampler.classes = ; loaded no samplers
[main] DEBUG org.apache.htrace.core.Tracer  - span.receiver.classes = ; loaded no span receivers
[main] DEBUG org.apache.hadoop.hdfs.client.impl.DfsClientConf  - dfs.client.use.legacy.blockreader.local = false
[main] DEBUG org.apache.hadoop.hdfs.client.impl.DfsClientConf  - dfs.client.read.shortcircuit = false
[main] DEBUG org.apache.hadoop.hdfs.client.impl.DfsClientConf  - dfs.client.domain.socket.data.traffic = false
[main] DEBUG org.apache.hadoop.hdfs.client.impl.DfsClientConf  - dfs.domain.socket.path = 
[main] DEBUG org.apache.hadoop.hdfs.DFSClient  - Sets dfs.client.block.write.replace-datanode-on-failure.min-replication to 0
[main] DEBUG org.apache.hadoop.io.retry.RetryUtils  - multipleLinearRandomRetry = null
[main] DEBUG org.apache.hadoop.ipc.Server  - rpcKind=RPC_PROTOCOL_BUFFER, rpcRequestWrapperClass=class org.apache.hadoop.ipc.ProtobufRpcEngine$RpcProtobufRequest, rpcInvoker=org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker@1b955cac
[main] DEBUG org.apache.hadoop.ipc.Client  - getting client out of cache: org.apache.hadoop.ipc.Client@289710d9
[main] DEBUG org.apache.hadoop.util.PerformanceAdvisory  - Both short-circuit local reads and UNIX domain socket are disabled.
[main] DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.sasl.DataTransferSaslUtil  - DataTransferProtocol not using SaslPropertiesResolver, no QOP found in configuration for dfs.data.transfer.protection
[main] DEBUG org.apache.hadoop.ipc.Client  - The ping interval is 60000 ms.
[main] DEBUG org.apache.hadoop.ipc.Client  - Connecting to localhost/127.0.0.1:9000
[IPC Client (689602108) connection to localhost/127.0.0.1:9000 from zonghuixu] DEBUG org.apache.hadoop.ipc.Client  - IPC Client (689602108) connection to localhost/127.0.0.1:9000 from zonghuixu: starting, having connections 1
[IPC Parameter Sending Thread #0] DEBUG org.apache.hadoop.ipc.Client  - IPC Client (689602108) connection to localhost/127.0.0.1:9000 from zonghuixu sending #0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations
[IPC Client (689602108) connection to localhost/127.0.0.1:9000 from zonghuixu] DEBUG org.apache.hadoop.ipc.Client  - IPC Client (689602108) connection to localhost/127.0.0.1:9000 from zonghuixu got value #0
[main] DEBUG org.apache.hadoop.io.retry.RetryInvocationHandler  - Exception while invoking call #0 ClientNamenodeProtocolTranslatorPB.getBlockLocations over null. Not retrying because try once and fail.
org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /user/set_java_env.sh
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:71)
	at org.apache.hadoop.hdfs.server.namenode.INodeFile.valueOf(INodeFile.java:61)
	at org.apache.hadoop.hdfs.server.namenode.FSDirStatAndListingOp.getBlockLocations(FSDirStatAndListingOp.java:152)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocations(FSNamesystem.java:1750)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getBlockLocations(NameNodeRpcServer.java:691)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.getBlockLocations(ClientNamenodeProtocolServerSideTranslatorPB.java:377)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:447)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:989)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:845)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:788)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1807)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2455)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1489)
	at org.apache.hadoop.ipc.Client.call(Client.java:1435)
	at org.apache.hadoop.ipc.Client.call(Client.java:1345)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:227)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy10.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.getBlockLocations(ClientNamenodeProtocolTranslatorPB.java:259)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:409)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:163)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:155)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:346)
	at com.sun.proxy.$Proxy11.getBlockLocations(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.callGetBlockLocations(DFSClient.java:847)
	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:836)
	at org.apache.hadoop.hdfs.DFSClient.getLocatedBlocks(DFSClient.java:825)
	at org.apache.hadoop.hdfs.DFSInputStream.fetchLocatedBlocksAndGetLastBlockLength(DFSInputStream.java:325)
	at org.apache.hadoop.hdfs.DFSInputStream.openInfo(DFSInputStream.java:285)
	at org.apache.hadoop.hdfs.DFSInputStream.<init>(DFSInputStream.java:270)
	at org.apache.hadoop.hdfs.DFSClient.open(DFSClient.java:1064)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:328)
	at org.apache.hadoop.hdfs.DistributedFileSystem$4.doCall(DistributedFileSystem.java:325)
	at org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)
	at org.apache.hadoop.hdfs.DistributedFileSystem.open(DistributedFileSystem.java:325)
	at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:787)
	at top.xvzonghui.hadoop.fs.FileOperations.cat(FileOperations.java:26)
	at top.xvzonghui.hadoop.fs.FileOperations.main(FileOperations.java:38)
[pool-4-thread-1] DEBUG org.apache.hadoop.ipc.Client  - stopping client from cache: org.apache.hadoop.ipc.Client@289710d9
[pool-4-thread-1] DEBUG org.apache.hadoop.ipc.Client  - removing client from cache: org.apache.hadoop.ipc.Client@289710d9
[pool-4-thread-1] DEBUG org.apache.hadoop.ipc.Client  - stopping actual client because no more references remain: org.apache.hadoop.ipc.Client@289710d9
[pool-4-thread-1] DEBUG org.apache.hadoop.ipc.Client  - Stopping client
[IPC Client (689602108) connection to localhost/127.0.0.1:9000 from zonghuixu] DEBUG org.apache.hadoop.ipc.Client  - IPC Client (689602108) connection to localhost/127.0.0.1:9000 from zonghuixu: closed
[IPC Client (689602108) connection to localhost/127.0.0.1:9000 from zonghuixu] DEBUG org.apache.hadoop.ipc.Client  - IPC Client (689602108) connection to localhost/127.0.0.1:9000 from zonghuixu: stopped, remaining connections 0
[Thread-2] DEBUG org.apache.hadoop.util.ShutdownHookManager  - ShutdownHookManger complete shutdown.
